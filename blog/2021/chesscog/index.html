<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>chess recognition | Georg W√∂lflein</title> <meta name="author" content="Georg W√∂lflein"/> <meta name="description" content="parsing chess game state from a picture of the board"/> <meta name="keywords" content="deep-learning, ai, ml, artificial-intelligence, machine-learning, digital-pathology"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üî•</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://georg.woelflein.eu/blog/2021/chesscog/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Georg¬†</span>W√∂lflein</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Georg_Wolflein_CV.pdf" target="_blank"> cv </a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">chess recognition</h1> <p class="post-meta">June 30, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> </p> </header> <article class="post-content"> <p>If you‚Äôre a recreational chess player, the following scenario will likely sound familiar: you‚Äôre plaing a casual over-the-board game, and reach an interesting position. You may even be thinking about a <a href="https://www.youtube.com/watch?v=G90SVhxKeig" target="_blank" rel="noopener noreferrer">spicy piece sacrifice</a>. Long story short, you want to perform a computer analysis after the game, to see if you made the right decision. So, you take a picture of the current position before proceeding with the game.</p> <p>After the game, you can use this picture to enter the position into a chess analysis program, like the popular <a href="https://lichess.org/analysis" target="_blank" rel="noopener noreferrer">analysis board tool on Lichess</a>. However, this process is time-consuming and error-prone ‚Äì you need to drag &amp; drop pieces around the board, until reaching the position you had in the photo.</p> <p>With all the recent (and not-so-recent) advances in deep learning and computer vision, you‚Äôd think it would be possible to automate this tedious procedure. Enter <em>chesscog</em>, an end-to-end chess recognition system I developed as part of my master‚Äôs thesis at the University of St Andrews and published as a journal article <a class="citation" href="#wolflein2021jimaging">[1]</a>. The goal of this project was to develop a system that is able to map a photo of a chess position to a structured format that can be understood by chess engines, such as the widely-used Forsyth-Edwards Notation (FEN). Perhaps the problem is best explained by a screenshot of the solution (a <a href="https://www.chesscog.com" target="_blank" rel="noopener noreferrer">demo app</a>) below.</p> <p><img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/demo_screenshot.png" data-zoomable=""></p> <div class="caption"> Screenshot of the <i>chesscog</i> <a href="https://www.chesscog.com" target="_blank" rel="noopener noreferrer">app</a>. </div> <p>How does it work? I‚Äôll give you the short version in this blog post; if you‚Äôre interested, check out my <a href="https://github.com/georg-wolflein/chesscog-report/raw/master/report.pdf" target="_blank" rel="noopener noreferrer">master‚Äôs thesis</a>, the associated <a href="http://mdpi.com/2313-433X/7/6/94" target="_blank" rel="noopener noreferrer">paper</a>, and the <a href="https://github.com/georg-wolflein/chesscog" target="_blank" rel="noopener noreferrer">code</a> on GitHub.</p> <h2 id="method">Method</h2> <p>When you think about the problem, it seems logical to decompose it into three steps:</p> <ol> <li> <p><strong>Board localisation.</strong> First, we need to localise the board, i.e. determine the four corner points. This is done using traditional computer vision techniques such as Canny edge detection <a class="citation" href="#canny1986">[2]</a>, Hough transform <a class="citation" href="#duda1972">[3]</a>, and RANSAC <a class="citation" href="#fischler1981">[4]</a>.</p> <p>Once the corner points are established, we can perform some simple geometric calculations to compute the nine horizontal and vertical grid lines, taking into account perspective distortion (the camera parameters are <em>not</em> required for this). It actually turns out that it is more convenient to remove the perspective distortion by projecting the localised chessboard onto a regular square grid.</p> <div class="row"> <div class="col-sm-7 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/3828_corners_unwarped_result.png" data-zoomable=""> </div> <div class="col-sm-5 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/3828_corners_warped_result.png" data-zoomable=""> </div> </div> <div class="caption"> The perspective distortion is removed from the left image by projecting the four corner points onto a square grid (right). </div> </li> <li> <p><strong>Occupancy classification.</strong> Next, we can crop out each of the chess squares from the un-distorted image, and feed them into a convolutional neural network (CNN) acting as a binary classifier between empty and occupied squares. It turns out that performing this step prior to the piece classification significantly increases accuracy (as opposed to considering the empty square as a piece type).</p> <p><img class="img-fluid rounded z-depth-1 center" src="/assets/blog/chesscog/occupancy_convnet.png" data-zoomable=""></p> <div class="caption"> Example of an occupancy classification CNN that distinguishes between empty and occupied squares. </div> </li> <li> <p><strong>Piece classification.</strong> Finally, the occupied samples are fed through another (larger) CNN that performs a 12-way classification to determine piece colour (black or white), and piece type (pawn, knight, bishop, rook, queen, or king). The predictions are gathered for each of the 64 squares and used to generate the corresponding FEN string.</p> </li> </ol> <h2 id="training">Training</h2> <p>In order to train the CNNs, a dataset is required. The lack of sizable and adequately labelled datasets is an issue recognised by several prior works <a class="citation" href="#ding2016">[5], [6], [7]</a>. Instead of manually creating a dataset and labelling it, I modelled a chess set in <a href="http://blender.org" target="_blank" rel="noopener noreferrer">Blender</a>, and created a Python script render ~5,000 synthetic chessboard images with different camera poses, lighting conditions, and game states. This approach allowed me to create a much larger dataset for chess recognition ‚Äì I made it publicly available <a href="http://osf.io/xf3ka/" target="_blank" rel="noopener noreferrer">here</a> <a class="citation" href="#wolflein2021">[8]</a>, to aid future research and enable fair comparison over a common dataset.</p> <p>I trained the occupancy classification and piece classification CNNs separately on the synthesised dataset. I tried out various hyperparameters and initialisations, and achieved the best results with models that were pre-traind on ImageNet. In the end, the system achieved an error rate of 0.23% per square on the test set, 28 times better than the previous state of the art <a class="citation" href="#mehta2020">[7]</a>.</p> <h2 id="adapting-to-unseen-chess-sets">Adapting to unseen chess sets</h2> <p>The last piece of the puzzle comes with the realisation that every chess set has a different appearance ‚Äì different shapes, materials, colours, etc. The pipeline described above does not work very well out-of-the-box on unseen real-life chess sets because it was only trained on synthetic images. So, how do we adapt to an unseen chess set?</p> <p>At this point it is important to note that the first step in the pipeline <em>does</em> actually work quite well on unseen chess sets, due to the use of traditional computer vision techniques (as opposed to learned features). This means that we can actually perform the perspective unwarping, and extract the 64 squares from practically any input image of a chess set.</p> <p>This lead me to the idea to ask the user to supply two pictures to fine-tune the CNNs to any new chess set. These two pictures should be of the starting position, once each players perspective. Using the board localisation algorithm, it turns out that just these two images can generate enough training data to fine-tune the two CNNs (keep in mind that each image generates 64 samples of chess squares), although careful data augmentations were required to achieve good results. The CNNs were initialised with the trained weights from the synthesised dataset, and then fine-tuned using the newly cropped and augmented images.</p> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/transfer_learning_white.png" data-zoomable=""> </div> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/transfer_learning_black.png" data-zoomable=""> </div> </div> <div class="caption"> An example of the two input images required to fine-tune the CNNs. </div> <p>In the evaluation, the fine-tuning dataset consisted of the two images depicted above, and the test dataset contained 27 images obtained by playing a game of chess and taking a picture after each move. Even on this unseen dataset, the chess recognition pipeline achieved very convincing results. The per-square misclassification rate was just 0.17% on the test set. See the table below for a summary of the results.</p> <p><img class="img-fluid rounded z-depth-1 center" src="/assets/blog/chesscog/results.png" data-zoomable=""></p> <div class="caption"> Performance of the chess recognition pipeline. </div> <h2 id="whats-next">What‚Äôs next?</h2> <p>There are plenty avenues to continue this research, below I‚Äôll name a few:</p> <ul> <li>Improve the process of adapting to unseen chess sets, for example by employing <a href="https://arxiv.org/abs/1703.03400" target="_blank" rel="noopener noreferrer">meta-learning</a> to reduce the number of steps needed to fine-tune the network.</li> <li>Train on a more diverse dataset of real-world images, to eliminate the need for fine-tuning entirely.</li> <li>Devise a differentiable pipeline that can be trained end-to-end (as opposed to the three steps outlined in this blog post which are trained separately).</li> <li>Improve the <a href="https://www.chesscog.com" target="_blank" rel="noopener noreferrer">web app</a> (it currently runs in <a href="http://heroku.com" target="_blank" rel="noopener noreferrer">Heroku</a>‚Äôs free tier on a single CPU, frequently running into out-of-memory errors).</li> </ul> <p>If you‚Äôre keen on helping on any of these points, or have other suggestions, drop me an email!</p> <h2 id="further-links">Further links</h2> <ul> <li>The <em>chesscog</em> <a href="https://www.chesscog.com" target="_blank" rel="noopener noreferrer">app</a>.</li> <li>The <a href="https://github.com/georg-wolflein/chesscog" target="_blank" rel="noopener noreferrer">code</a> is available on GitHub.</li> <li>My <a href="http://mdpi.com/2313-433X/7/6/94" target="_blank" rel="noopener noreferrer">article</a> in the <em>Journal of Imaging</em>, ‚ÄúDetermining Chess Game State From an Image‚Äù.</li> <li>My <a href="https://github.com/georg-wolflein/chesscog-report/raw/master/report.pdf" target="_blank" rel="noopener noreferrer">master‚Äôs thesis</a>, which goes into more detail than this blog post and the journal article.</li> </ul> <hr> <h2>References</h2> <ol class="bibliography"> <li><span id="wolflein2021">G. W√∂lflein and O. Arandjeloviƒá, ‚ÄúDataset of Rendered Chess Game State Images.‚Äù Open Science Framework, 2021. doi: 10.17605/OSF.IO/XF3KA.</span></li> <li><span id="wolflein2021jimaging">G. W√∂lflein and O. Arandjeloviƒá, ‚ÄúDetermining Chess Game State from an Image,‚Äù <i>Journal of Imaging</i>, vol. 7, no. 6, 2021.</span></li> <li><span id="czyzewski2020">M. A. Czyzewski, A. Laskowski, and S. Wasik, ‚ÄúChessboard and Chess Piece Recognition with the Support of Neural Networks.‚Äù 2020.</span></li> <li><span id="mehta2020">A. Mehta and H. Mehta, ‚ÄúAugmented Reality Chess Analyzer (ARChessAnalyzer),‚Äù <i>Journal of Emerging Investigators</i>, vol. 2, 2020.</span></li> <li><span id="ding2016">J. Ding, ‚ÄúChessVision: Chess Board and Piece Recognition.‚Äù Stanford University, 2016.</span></li> <li><span id="canny1986">J. Canny, ‚ÄúA Computational Approach to Edge Detection,‚Äù <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 1986.</span></li> <li><span id="fischler1981">M. A. Fischler and R. C. Bolles, ‚ÄúRandom Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography,‚Äù <i>Communications of the ACM</i>, vol. 24, no. 6, 1981.</span></li> <li><span id="duda1972">R. O. Duda and P. E. Hart, ‚ÄúUse of the Hough Transformation to Detect Lines and Curves in Pictures,‚Äù <i>Communications of the ACM</i>, vol. 15, no. 1, 1972.</span></li> </ol> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2024 Georg W√∂lflein. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>