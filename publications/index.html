<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Georg WÃ¶lflein</title> <meta name="author" content="Georg WÃ¶lflein"/> <meta name="description" content="Georg WÃ¶lflein's homepage. "/> <meta name="keywords" content="deep-learning, ai, ml, artificial-intelligence, machine-learning, digital-pathology"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://georg.woelflein.eu/publications/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">GeorgÂ </span>WÃ¶lflein</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Georg_Wolflein_CV.pdf" target="_blank"> cv </a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://www.nature.com/articles/s41467-024-51465-9" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/multimodal-incontext.png"> <abbr class="badge">Nat Commun</abbr> </a> </div> <div id="ferber2024incontext" class="col-sm-7"> <div class="title"> In-context learning enables multimodal large language models to classify cancer pathology images </div> <div class="author"> <a href="https://www.klinikum.uni-heidelberg.de/personen/dyke-ferber-9789" target="_blank" rel="noopener noreferrer">Dyke Ferber</a>, <em>Georg WÃ¶lflein</em>, <a href="https://scholar.google.com/citations?user=OtcWA9IAAAAJ" target="_blank" rel="noopener noreferrer">Isabella C. Wiest</a>, <a href="https://scholar.google.com/citations?user=0LLfhWYAAAAJ" target="_blank" rel="noopener noreferrer">Marta Ligero</a>, Srividhya Sainath, Narmin Ghaffari Laleh, <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, Gustav MÃ¼ller-Franzes, Dirk JÃ¤ger, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> <em>Nature Communications</em>, vol. 15, no. 1, 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.07407" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.nature.com/articles/s41467-024-51465-9.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Dyke-F/GPT-4V-In-Context-Learning" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Medical image classification requires labeled, task-specific datasets which are used to train deep learning networks de novo, or to fine-tune foundation models. However, this process is computationally and technically demanding. In language processing, in-context learning provides an alternative, where models learn from within prompts, bypassing the need for parameter updates. Yet, in-context learning remains underexplored in medical image analysis. Here, we systematically evaluate the model Generative Pretrained Transformer 4 with Vision capabilities (GPT-4V) on cancer image processing with in-context learning on three cancer histopathology tasks of high importance: Classification of tissue subtypes in colorectal cancer, colon polyp subtyping and breast tumor detection in lymph node sections. Our results show that in-context learning is sufficient to match or even outperform specialized neural networks trained for particular tasks, while only requiring a minimal number of samples. In summary, this study demonstrates that large vision language models trained on non-domain specific data can be applied out-of-the box to solve medical image-processing tasks in histopathology. This democratizes access of generalist AI models to medical experts without technical background especially for areas where annotated data is scarce.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://arxiv.org/abs/2411.13623" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/cobra.png"> </a> </div> <div id="lenz2024cobra" class="col-sm-7"> <div class="title"> Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning </div> <div class="author"> <a href="https://scholar.google.com/citations?user=4S75gC0AAAAJ" target="_blank" rel="noopener noreferrer">Tim Lenz</a>, Peter Neidlinger, <a href="https://scholar.google.com/citations?user=0LLfhWYAAAAJ" target="_blank" rel="noopener noreferrer">Marta Ligero</a>, <em>Georg WÃ¶lflein</em>, Marko van Treeck, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.13623" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2411.13623.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Representation learning of pathology whole-slide images (WSIs) has primarily relied on weak supervision with Multiple Instance Learning (MIL). This approach leads to slide representations highly tailored to a specific clinical task. Self-supervised learning (SSL) has been successfully applied to train histopathology foundation models (FMs) for patch embedding generation. However, generating patient or slide level embeddings remains challenging. Existing approaches for slide representation learning extend the principles of SSL from patch level learning to entire slides by aligning different augmentations of the slide or by utilizing multimodal data. By integrating tile embeddings from multiple FMs, we propose a new single modality SSL method in feature space that generates useful slide representations. Our contrastive pretraining strategy, called COBRA, employs multiple FMs and an architecture based on Mamba-2. COBRA exceeds performance of state-of-the-art slide encoders on four different public CPTAC cohorts on average by at least +3.8% AUC, despite only being pretrained on 3048 WSIs from TCGA. Additionally, COBRA is readily compatible at inference time with previously unseen feature extractors.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://arxiv.org/abs/2411.16803" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/clear.png"> </a> </div> <div id="ligero2024clear" class="col-sm-7"> <div class="title"> Abnormality-Driven Representation Learning for Radiology Imaging </div> <div class="author"> <a href="https://scholar.google.com/citations?user=0LLfhWYAAAAJ" target="_blank" rel="noopener noreferrer">Marta Ligero</a>, <a href="https://scholar.google.com/citations?user=4S75gC0AAAAJ" target="_blank" rel="noopener noreferrer">Tim Lenz</a>, <em>Georg WÃ¶lflein</em>, <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.16803" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2411.16803.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>To date, the most common approach for radiology deep learning pipelines is the use of end-to-end 3D networks based on models pre-trained on other tasks, followed by fine-tuning on the task at hand. In contrast, adjacent medical fields such as pathology, which focus on 2D images, have effectively adopted task-agnostic foundational models based on self-supervised learning (SSL), combined with weakly-supervised deep learning (DL). However, the field of radiology still lacks task-agnostic representation models due to the computational and data demands of 3D imaging and the anatomical complexity inherent to radiology scans. To address this gap, we propose CLEAR, a framework for radiology images that uses extracted embeddings from 2D slices along with attention-based aggregation for efficiently predicting clinical endpoints. As part of this framework, we introduce lesion-enhanced contrastive learning (LeCL), a novel approach to obtain visual representations driven by abnormalities in 2D axial slices across different locations of the CT scans. Specifically, we trained single-domain contrastive learning approaches using three different architectures: Vision Transformers, Vision State Space Models and Gated Convolutional Neural Networks. We evaluate our approach across three clinical tasks: tumor lesion location, lung disease detection, and patient staging, benchmarking against four state-of-the-art foundation models, including BiomedCLIP. Our findings demonstrate that CLEAR using representations learned through LeCL, outperforms existing foundation models, while being substantially more compute- and data-efficient.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://link.springer.scom/chapter/10.1007/978-3-031-72083-3_24" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/joint-mtl.png"> <abbr class="badge">MICCAI</abbr> </a> </div> <div id="elnahhas2024joint" class="col-sm-7"> <div class="title"> Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology </div> <div class="author"> <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, <em>Georg WÃ¶lflein</em>, <a href="https://scholar.google.com/citations?user=0LLfhWYAAAAJ" target="_blank" rel="noopener noreferrer">Marta Ligero</a>, <a href="https://scholar.google.com/citations?user=4S75gC0AAAAJ" target="_blank" rel="noopener noreferrer">Tim Lenz</a>, Marko van Treeck, <a href="https://scholar.google.com/citations?user=to8g5LsAAAAJ" target="_blank" rel="noopener noreferrer">Firas Khader</a>, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> <em>In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>, 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.03891" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://papers.miccai.org/miccai-2024/paper/1368_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Avic3nna/joint-mtl-cpath" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a weakly-supervised setting. Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest. Nonetheless, clinical decision making often requires a categorical outcome. Consequently, we developed a weakly-supervised joint multi-task Transformer architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment. Moreover, we perform a comprehensive benchmark of 16 approaches of task balancing for weakly-supervised joint multi-task learning in computational pathology. Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7% and +4.1%, as well as yielding better clustering of latent embeddings by +8% and +5% for the prediction of MSI and HRD in external cohorts, respectively.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://www.nature.com/articles/s41596-024-01047-2" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/stamp.png"> <abbr class="badge">Nat Protoc</abbr> </a> </div> <div id="elnahhas2023stamp" class="col-sm-7"> <div class="title"> From Whole-slide Image to Biomarker Prediction: End-to-End Weakly Supervised Deep Learning in Computational Pathology </div> <div class="author"> <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, Marko van Treeck, <em>Georg WÃ¶lflein</em>, <a href="https://scholar.google.com/citations?user=qTtTRjgAAAAJ" target="_blank" rel="noopener noreferrer">Michaela Unger</a>, <a href="https://scholar.google.com/citations?user=0LLfhWYAAAAJ" target="_blank" rel="noopener noreferrer">Marta Ligero</a>, <a href="https://scholar.google.com/citations?user=4S75gC0AAAAJ" target="_blank" rel="noopener noreferrer">Tim Lenz</a>, <a href="https://scholar.google.de/citations?user=skcoUZMAAAAJ" target="_blank" rel="noopener noreferrer">Sophia J. Wagner</a>, <a href="https://orcid.org/0000-0001-6602-0141" target="_blank" rel="noopener noreferrer">Katherine J. Hewitt</a>, <a href="https://scholar.google.com/citations?user=to8g5LsAAAAJ" target="_blank" rel="noopener noreferrer">Firas Khader</a>, Sebastian Foersch, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> <em>Nature Protocols</em>, 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.10944" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2312.10944.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/KatherLab/STAMP" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Hematoxylin- and eosin-stained whole-slide images (WSIs) are the foundation of diagnosis of cancer. In recent years, development of deep learning-based methods in computational pathology has enabled the prediction of biomarkers directly from WSIs. However, accurately linking tissue phenotype to biomarkers at scale remains a crucial challenge for democratizing complex biomarkers in precision oncology. This protocol describes a practical workflow for solid tumor associative modeling in pathology (STAMP), enabling prediction of biomarkers directly from WSIs by using deep learning. The STAMP workflow is biomarker agnostic and allows for genetic and clinicopathologic tabular data to be included as an additional input, together with histopathology images. The protocol consists of five main stages that have been successfully applied to various research problems: formal problem definition, data preprocessing, modeling, evaluation and clinical translation. The STAMP workflow differentiates itself through its focus on serving as a collaborative framework that can be used by clinicians and engineers alike for setting up research projects in the field of computational pathology. As an example task, we applied STAMP to the prediction of microsatellite instability (MSI) status in colorectal cancer, showing accurate performance for the identification of tumors high in MSI. Moreover, we provide an open-source code base, which has been deployed at several hospitals across the globe to set up computational pathology workflows. The STAMP workflow requires one workday of hands-on computational execution and basic command line knowledge.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://arxiv.org/abs/2311.11772v4" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/histaug.png"> <abbr class="badge">ECCV</abbr> </a> </div> <div id="wolflein2023good" class="col-sm-7"> <div class="title"> A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://www.klinikum.uni-heidelberg.de/personen/dyke-ferber-9789" target="_blank" rel="noopener noreferrer">Dyke Ferber</a>, <a href="https://scholar.google.com/citations?user=GnhtOGMAAAAJ" target="_blank" rel="noopener noreferrer">Asier Rabasco Meneghetti</a>, <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, <a href="https://scholar.google.com/citations?user=I_G3fPgAAAAJ" target="_blank" rel="noopener noreferrer">Zunamys I. Carrero</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J. Harrison</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em>, 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.11772" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2311.11772v4.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/good-features" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.youtube.com/watch?v=Tst4XtaT9RE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://georg.woelflein.eu/good-features" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Stain normalisation is thought to be a crucial preprocessing step in computational pathology pipelines. We question this belief in the context of weakly supervised whole slide image classification, motivated by the emergence of powerful feature extractors trained using self-supervised learning on diverse pathology datasets. To this end, we performed the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 8,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Notably, we find that omitting stain normalisation and image augmentations does not compromise downstream slide-level classification performance, while incurring substantial savings in memory and compute. Using a new evaluation metric that facilitates relative downstream performance comparison, we identify the best publicly available extractors, and show that their latent spaces are remarkably robust to variations in stain and augmentations like rotation. Contrary to previous patch-level benchmarking studies, our approach emphasises clinical relevance by focusing on slide-level biomarker prediction tasks in a weakly supervised setting with external validation cohorts. Our findings stand to streamline digital pathology workflows by minimising preprocessing needs and informing the selection of feature extractors.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://arxiv.org/abs/2407.13463" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/trial-matching.png"> </a> </div> <div id="ferber2024trialmatching" class="col-sm-7"> <div class="title"> End-To-End Clinical Trial Matching with Large Language Models </div> <div class="author"> <a href="https://www.klinikum.uni-heidelberg.de/personen/dyke-ferber-9789" target="_blank" rel="noopener noreferrer">Dyke Ferber</a>, Lars Hilgers, Isabella C Wiest, Marie-Elisabeth LeÃŸmann, Jan Clusmann, Peter Neidlinger, Jiefu Zhu, <em>Georg WÃ¶lflein</em>, Jacqueline Lammert, Maximilian Tschochohei, Heiko BÃ¶hme, Dirk JÃ¤ger, Mihaela Aldea, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, Christiane HÃ¶per, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.13463" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2407.13463.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Matching cancer patients to clinical trials is essential for advancing treatment and patient care. However, the inconsistent format of medical free text documents and complex trial eligibility criteria make this process extremely challenging and time-consuming for physicians. We investigated whether the entire trial matching process â€“ from identifying relevant trials among 105,600 oncology-related clinical trials on clinicaltrials.gov to generating criterion-level eligibility matches â€“ could be automated using Large Language Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic Health Records (EHRs), we demonstrate that our approach identifies relevant candidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0% when matching patient-level information at the criterion level against a baseline defined by human experts. Utilizing LLM feedback reveals that 39.3% criteria that were initially considered incorrect are either ambiguous or inaccurately annotated, leading to a total model accuracy of 92.7% after refining our human baseline. In summary, we present an end-to-end pipeline for clinical trial matching using LLMs, demonstrating high precision in screening and matching trials to individual patients, even outperforming the performance of qualified medical doctors. Our fully end-to-end pipeline can operate autonomously or with human supervision and is not restricted to oncology, offering a scalable solution for enhancing patient-trial matching in real-world settings.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://arxiv.org/abs/2311.11772v5" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/histaug.png"> </a> </div> <div id="wolflein2024benchmarking" class="col-sm-7"> <div class="title"> Benchmarking Pathology Feature Extractors for Whole Slide Image Classification </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://www.klinikum.uni-heidelberg.de/personen/dyke-ferber-9789" target="_blank" rel="noopener noreferrer">Dyke Ferber</a>, <a href="https://scholar.google.com/citations?user=GnhtOGMAAAAJ" target="_blank" rel="noopener noreferrer">Asier Rabasco Meneghetti</a>, <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, <a href="https://scholar.google.com/citations?user=I_G3fPgAAAAJ" target="_blank" rel="noopener noreferrer">Zunamys I. Carrero</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J. Harrison</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.11772v5" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2311.11772v5.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/good-features" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.youtube.com/watch?v=Tst4XtaT9RE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://georg.woelflein.eu/good-features" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Weakly supervised whole slide image classification is a key task in computational pathology, which involves predicting a slide-level label from a set of image patches constituting the slide. Constructing models to solve this task involves multiple design choices, often made without robust empirical or conclusive theoretical justification. To address this, we conduct a comprehensive benchmarking of feature extractors to answer three critical questions: 1) Is stain normalisation still a necessary preprocessing step? 2) Which feature extractors are best for downstream slide-level classification? 3) How does magnification affect downstream performance? Our study constitutes the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 10,000 training runs across 14 feature extractors, 9 tasks, 5 datasets, 3 downstream architectures, 2 levels of magnification, and various preprocessing setups. Our findings challenge existing assumptions: 1) We observe empirically, and by analysing the latent space, that skipping stain normalisation and image augmentations does not degrade performance, while significantly reducing memory and computational demands. 2) We develop a novel evaluation metric to compare relative downstream performance, and show that the choice of feature extractor is the most consequential factor for downstream performance. 3) We find that lower-magnification slides are sufficient for accurate slide-level classification. Contrary to previous patch-level benchmarking studies, our approach emphasises clinical relevance by focusing on slide-level biomarker prediction tasks in a weakly supervised setting with external validation cohorts. Our findings stand to streamline digital pathology workflows by minimising preprocessing needs and informing the selection of feature extractors.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://ai.nejm.org/doi/abs/10.1056/AIcs2300235" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/guidelines-rag.png"> <abbr class="badge">NEJM AI</abbr> </a> </div> <div id="ferber2024gpt4" class="col-sm-7"> <div class="title"> GPT-4 for Information Retrieval and Comparison of Medical Oncology Guidelines </div> <div class="author"> <a href="https://www.klinikum.uni-heidelberg.de/personen/dyke-ferber-9789" target="_blank" rel="noopener noreferrer">Dyke Ferber</a>, <a href="https://scholar.google.com/citations?user=OtcWA9IAAAAJ" target="_blank" rel="noopener noreferrer">Isabella C. Wiest</a>, <em>Georg WÃ¶lflein</em>, Matthias P. Ebert, Gernot Beutel, Jan-Niklas Eckardt, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, Christoph Springfeld, Dirk JÃ¤ger, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> <em>NEJM AI</em>, vol. 1, no. 6, 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ai.nejm.org/stoken/default+domain/NCV9YRPAYCBHPZN7UA7G/full?redirectUri=/doi/full/10.1056/AIcs2300235" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Full text</a> <a href="https://github.com/Dyke-F/RAG_Medical_Guidelines" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Oncologists face increasingly complex clinical decision-making processes as new cancer therapies are approved and treatment guidelines are revised at an unprecedented rate. With the aim of improving oncologistsâ€™ efficiency and supporting their adherence to the most recent treatment recommendations, we evaluated the use of the large language model generative pretrained transformer 4 (GPT-4) to interpret guidelines from the American Society of Clinical Oncology and the European Society for Medical Oncology. The ability of GPT-4 to answer clinically relevant questions regarding the management of patients with pancreatic cancer, metastatic colorectal cancer, and hepatocellular carcinoma was assessed. We also assessed GPT-4 outputs with and without retrieval-augmented generation (RAG), which provided additional knowledge to the model, and then manually compared the results with the original guideline documents. GPT-4 with RAG provided correct responses in 84% of cases (of 218 statements, 184 were correct, 30 were inaccurate, and 4 were wrong). GPT-4 without RAG provided correct responses in only 57% of cases (of 163 statements, 93 were correct, 29 were inaccurate, and 41 were wrong). We showed that GPT-4, when enhanced with additional clinical information through RAG, can accurately identify detailed similarities and disparities in diagnostic and treatment proposals across different authoritative sources. Generative pretrained transformer 4, together with retrieval-augmented generation, can precisely extract and compare medical guideline recommendations in oncology from different associations. </p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://arxiv.org/abs/2404.04667" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/rag-agents.png"> </a> </div> <div id="ferber2024autonomous" class="col-sm-7"> <div class="title"> Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology </div> <div class="author"> <a href="https://www.klinikum.uni-heidelberg.de/personen/dyke-ferber-9789" target="_blank" rel="noopener noreferrer">Dyke Ferber</a>, <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, <em>Georg WÃ¶lflein</em>, <a href="https://scholar.google.com/citations?user=OtcWA9IAAAAJ" target="_blank" rel="noopener noreferrer">Isabella C. Wiest</a>, Jan Clusmann, Marie-Elisabeth LeÃŸman, Sebastian Foersch, Jacqueline Lammert, Maximilian Tschochohei, Dirk JÃ¤ger, Manuel Salto-Tellez, Nikolaus Schultz, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.04667" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2404.04667.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Multimodal artificial intelligence (AI) systems have the potential to enhance clinical decision-making by interpreting various types of medical data. However, the effectiveness of these models across all medical fields is uncertain. Each discipline presents unique challenges that need to be addressed for optimal performance. This complexity is further increased when attempting to integrate different fields into a single model. Here, we introduce an alternative approach to multimodal medical AI that utilizes the generalist capabilities of a large language model (LLM) as a central reasoning engine. This engine autonomously coordinates and deploys a set of specialized medical AI tools. These tools include text, radiology and histopathology image interpretation, genomic data processing, web searches, and document retrieval from medical guidelines. We validate our system across a series of clinical oncology scenarios that closely resemble typical patient care workflows. We show that the system has a high capability in employing appropriate tools (97%), drawing correct conclusions (93.6%), and providing complete (94%), and helpful (89.2%) recommendations for individual patient cases while consistently referencing relevant literature (82.5%) upon instruction. This work provides evidence that LLMs can effectively plan and execute domain-specific models to retrieve or synthesize new information when used as autonomous agents. This enables them to function as specialist, patient-tailored clinical assistants. It also simplifies regulatory compliance by allowing each component tool to be individually validated and approved. We believe, that our work can serve as a proof-of-concept for more advanced LLM-agents in the medical domain.</p> </div> </div> </div></li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://zenodo.org/records/8214502" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hoechstgan.png"> <abbr class="badge">ISMB/ECCB</abbr> </a> </div> <div id="alouges2023performance" class="col-sm-7"> <div class="title"> Performance comparison between federated and centralized learning with a deep learning model on Hoechst stained images </div> <div class="author"> Damien Alouges, <em>Georg WÃ¶lflein</em>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/in-hwa-um(0ac978a2-6ef8-4397-bc36-f920a77696a3).html" target="_blank" rel="noopener noreferrer">In Hwa Um</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David Harrison</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a>, <a href="https://scholar.google.com/citations?user=at9bmQQAAAAJ" target="_blank" rel="noopener noreferrer">Christophe Battail</a>, and <a href="https://dblp.org/pid/184/8678.html" target="_blank" rel="noopener noreferrer">StÃ©phane Gazut</a> </div> <div class="periodical"> <em>ISMB/ECCB Abstracts</em>, 2023. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://zenodo.org/records/8214502/files/Alouges_ECCB-ISMB2023.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Medical data is not fully exploited by Machine Learning (ML) techniques because the privacy concerns restrict the sharing of sensitive information and consequently the use of centralized ML schemes. Usually, ML models trained on local data are failing to reach their full potential owing to low statistical power. Federated Learning (FL) solves critical issues in the healthcare domain such as data privacy and enables multiple contributors to build a common and robust ML model by sharing local learning parameters without sharing data. FL approaches are mainly evaluated in the literature using benchmarks and the trade-off between accuracy and privacy still has to be more studied in a realistic clinical context. In this work, part of the European project KATY (GA:101017453), we evaluate this trade-off for a CD3/CD8 cells staining procedure from Hoechst images. WÃ¶lflein et al. developed a deep learning GAN model that synthesizes CD3 and CD8 stains from kidney cancer tissue slides, trained on 473,000 patches (256x256 pixels) from 8 whole slide images. We modified the training to simulate a FL approach by distributing the learning across 8 clients and aggregating the parameters to create the overall model. We present then the performance comparison between FL and centralized learning.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://arxiv.org/abs/2305.10552" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/das-mil.png"> </a> </div> <div id="wolflein2023deep" class="col-sm-7"> <div class="title"> Deep Multiple Instance Learning with Distance-Aware Self-Attention </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://caraml-group.github.io/author/lucie-charlotte-magister/" target="_blank" rel="noopener noreferrer">Lucie Charlotte Magister</a>, <a href="https://www.cl.cam.ac.uk/~pl219/" target="_blank" rel="noopener noreferrer">Pietro LiÃ²</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J Harrison</a>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> 2023. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.10552" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2305.10552.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/das-mil" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Traditional supervised learning tasks require a label for every instance in the training set, but in many real-world applications, labels are only available for collections (bags) of instances. This problem setting, known as multiple instance learning (MIL), is particularly relevant in the medical domain, where high-resolution images are split into smaller patches, but labels apply to the image as a whole. Recent MIL models are able to capture correspondences between patches by employing self-attention, allowing them to weigh each patch differently based on all other patches in the bag. However, these approaches still do not consider the relative spatial relationships between patches within the larger image, which is especially important in computational pathology. To this end, we introduce a novel MIL model with distance-aware self-attention (DAS-MIL), which explicitly takes into account relative spatial information when modelling the interactions between patches. Unlike existing relative position representations for self-attention which are discrete, our approach introduces continuous distance-dependent terms into the computation of the attention weights, and is the first to apply relative position representations in the context of MIL. We evaluate our model on a custom MNIST-based MIL dataset that requires the consideration of relative spatial information, as well as on CAMELYON16, a publicly available cancer metastasis detection dataset, where we achieve a test AUROC score of 0.91. On both datasets, our model outperforms existing MIL approaches that employ absolute positional encodings, as well as existing relative position representation schemes applied to MIL. Our code is available at https://anonymous.4open.science/r/das-mil.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://www.mdpi.com/2306-5729/8/2/40" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hoechstgan-dataset.png"> <abbr class="badge">Data</abbr> </a> </div> <div id="wolflein2023data" class="col-sm-7"> <div class="title"> Whole-Slide Images and Patches of Clear Cell Renal Cell Carcinoma Tissue Sections Counterstained with Hoechst 33342, CD3, and CD8 Using Multiple Immunofluorescence </div> <div class="author"> <em>Georg WÃ¶lflein</em>Â *, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/in-hwa-um(0ac978a2-6ef8-4397-bc36-f920a77696a3).html" target="_blank" rel="noopener noreferrer">In Hwa Um</a>Â *, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J Harrison</a>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> <em>Data</em>, vol. 8, no. 2, 2023. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.mdpi.com/2306-5729/8/2/40/pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, there has been an increased effort to digitise whole-slide images of cancer tissue. This effort has opened up a range of new avenues for the application of deep learning in oncology. One such avenue is virtual staining, where a deep learning model is tasked with reproducing the appearance of stained tissue sections, conditioned on a different, often times less expensive, input stain. However, data to train such models in a supervised manner where the input and output stains are aligned on the same tissue sections are scarce. In this work, we introduce a dataset of ten whole-slide images of clear cell renal cell carcinoma tissue sections counterstained with Hoechst 33342, CD3, and CD8 using multiple immunofluorescence. We also provide a set of over 600,000 patches of size 256 Ã— 256 pixels extracted from these images together with cell segmentation masks in a format amenable to training deep learning models. It is our hope that this dataset will be used to further the development of deep learning methods for digital pathology by serving as a dataset for comparing and benchmarking virtual staining models.</p> </div> <span style="font-size: smaller;">*Â equal contribution</span> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://openaccess.thecvf.com/content/WACV2023/html/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.html" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hoechstgan.png"> <abbr class="badge">WACV</abbr> </a> </div> <div id="wolflein2023hoechstgan" class="col-sm-7"> <div class="title"> HoechstGAN: Virtual Lymphocyte Staining Using Generative Adversarial Networks </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/in-hwa-um(0ac978a2-6ef8-4397-bc36-f920a77696a3).html" target="_blank" rel="noopener noreferrer">In Hwa Um</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J Harrison</a>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2023. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.06909" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/hoechstgan" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/hoechstgan_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a> <a href="https://www.youtube.com/watch?v=XrhwqRNjtv4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://georg.woelflein.eu/hoechstgan" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The presence and density of specific types of immune cells are important to understand a patientâ€™s immune response to cancer. However, immunofluorescence staining required to identify T cell subtypes is expensive, timeconsuming, and rarely performed in clinical settings. We present a framework to virtually stain Hoechst images (which are cheap and widespread) with both CD3 and CD8 to identify T cell subtypes in clear cell renal cell carcinoma using generative adversarial networks. Our proposed method jointly learns both staining tasks, incentivising the network to incorporate mutually beneficial information from each task. We devise a novel metric to quantify the virtual staining quality, and use it to evaluate our method.</p> </div> </div> </div></li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://www.mdpi.com/2072-6694/14/21/5387" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/highplex.png"> <abbr class="badge">Cancers</abbr> </a> </div> <div id="defilippis2022use" class="col-sm-7"> <div class="title"> Use of High-Plex Data Reveals Novel Insights into the Tumour Microenvironment of Clear Cell Renal Cell Carcinoma </div> <div class="author"> Raffaele De FilippisÂ *, <em>Georg WÃ¶lflein</em>Â *, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/in-hwa-um(0ac978a2-6ef8-4397-bc36-f920a77696a3).html" target="_blank" rel="noopener noreferrer">In Hwa Um</a>, Peter D Caie, Sarah Warren, Andrew White, Elizabeth Suen, Emily To, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J Harrison</a> </div> <div class="periodical"> <em>Cancers</em>, vol. 14, no. 21, 2022. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://medrxiv.org/content/10.1101/2022.10.13.22281035" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">medRxiv</a> <a href="https://www.mdpi.com/2072-6694/14/21/5387/pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Although immune checkpoint inhibitors (ICIs) have significantly improved the oncological outcomes, about one-third of patients affected by clear cell renal cell carcinoma (ccRCC) still experience recurrence. Current prognostic algorithms, such as the Leibovich score (LS), rely on morphological features manually assessed by pathologists and are therefore subject to bias. Moreover, these tools do not consider the heterogeneous molecular milieu present in the Tumour Microenvironment (TME), which may have prognostic value. We systematically developed a semi-automated method to investigate 62 markers and their combinations in 150 primary ccRCCs using Multiplex Immunofluorescence (mIF), NanoString GeoMxÂ® Digital Spatial Profiling (DSP) and Artificial Intelligence (AI)-assisted image analysis in order to find novel prognostic signatures and investigate their spatial relationship. We found that coexpression of cancer stem cell (CSC) and epithelial-to-mesenchymal transition (EMT) markers such as OCT4 and ZEB1 are indicative of poor outcome. OCT4 and the immune markers CD8, CD34, and CD163 significantly stratified patients at intermediate LS. Furthermore, augmenting the LS with OCT4 and CD34 improved patient stratification by outcome. Our results support the hypothesis that combining molecular markers has prognostic value and can be integrated with morphological features to improve risk stratification and personalised therapy. To conclude, GeoMxÂ® DSP and AI image analysis are complementary tools providing high multiplexing capability required to investigate the TME of ccRCC, while reducing observer bias.</p> </div> <span style="font-size: smaller;">*Â equal contribution</span> </div> </div></li></ol> <h2 class="year">2021</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://www.mdpi.com/2313-433X/7/6/94" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/chesscog.png"> <abbr class="badge">J. Imaging</abbr> </a> </div> <div id="wolflein2021determining" class="col-sm-7"> <div class="title"> Determining Chess Game State from an Image </div> <div class="author"> <em>Georg WÃ¶lflein</em>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> <em>Journal of Imaging</em>, vol. 7, no. 6, 2021. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2104.14963" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.mdpi.com/2313-433X/7/6/94/pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/blog/2021/chesscog" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/georg-wolflein/chesscog" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Identifying the configuration of chess pieces from an image of a chessboard is a problem in computer vision that has not yet been solved accurately. However, it is important for helping amateur chess players improve their games by facilitating automatic computer analysis without the overhead of manually entering the pieces. Current approaches are limited by the lack of large datasets and are not designed to adapt to unseen chess sets. This paper puts forth a new dataset synthesised from a 3D model that is an order of magnitude larger than existing ones. Trained on this dataset, a novel end-to-end chess recognition system is presented that combines traditional computer vision techniques with deep learning. It localises the chessboard using a RANSAC-based algorithm that computes a projective transformation of the board onto a regular grid. Using two convolutional neural networks, it then predicts an occupancy mask for the squares in the warped image and finally classifies the pieces. The described system achieves an error rate of 0.23% per square on the test set, 28 times better than the current state of the art. Further, a few-shot transfer learning approach is developed that is able to adapt the inference system to a previously unseen chess set using just two photos of the starting position, obtaining a per-square accuracy of 99.83% on images of that new chess set. The code, dataset, and trained models are made available online.</p> </div> </div> </div></li></ol> <h1>datasets</h1> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://www.ebi.ac.uk/biostudies/bioimages/studies/S-BIAD605" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hoechstgan-dataset.png"> </a> </div> <div id="wolflein2022bioimages" class="col-sm-7"> <div class="title"> Whole slide images and patches of clear cell renal cell carcinoma counterstained with multiple immunofluorescence for Hoechst, CD3, and CD8 </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/in-hwa-um(0ac978a2-6ef8-4397-bc36-f920a77696a3).html" target="_blank" rel="noopener noreferrer">In Hwa Um</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J Harrison</a>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> <em>BioImage Archive</em>, 2022. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.ebi.ac.uk/biostudies/bioimages/studies/S-BIAD605" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>We provide a dataset of 10 whole slide images of clear cell renal cell carcinoma tissue sections that were counterstained with Hoechst 33342, Cy3 and Cy5. The fluorescent images were captured using Zeiss Axio Scan Z1. We used three different fluorescent channels (Hoechst 33342, Cy3 and Cy5) simultaneously to capture individual channel images under 20x object magnification with respective exposure times of 10 ms, 20 ms and 30 ms. We also provide clinical data including age at surgery, gender, disease free months, nuclear grade, and Leibovich score. Alongside the WSIs, we provide pre-processed and normalised patches (each 256x256 pixels) from the WSIs in a format that can be readily ingested by deep learning models for their training. For each patch, we provide normalised Hoechst, CD3, and CD8 images, as well as cell masks identified using the StarDist algorithm. Each cell is annotated as CD3, CD8 (a subset of CD3), or neither.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://osf.io/xf3ka" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/chesscog-dataset.png"> </a> </div> <div id="wolflein2021osf" class="col-sm-7"> <div class="title"> Dataset of Rendered Chess Game State Images </div> <div class="author"> <em>Georg WÃ¶lflein</em>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> <em>Open Science Foundation</em>, 2021. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://osf.io/xf3ka" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>This dataset contains 4,888 synthetic images of chess game states that occurred in games played by Magnus Carlsen. The images were rendered in Blender at different angles and lighting conditions.</p> </div> </div> </div></li> </ol> <h1>theses</h1> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://github.com/georg-wolflein/chesscog" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/chesscog.png"> <abbr class="badge">MSci</abbr> </a> </div> <div id="wolflein2021msci" class="col-sm-7"> <div class="title"> Determining Chess Game State From an Image Using Machine Learning </div> <div class="author"> <em>Georg WÃ¶lflein</em> (supervised by <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a>) </div> <div class="periodical"> <em>University of St Andrews</em>, 2021. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/georg-wolflein/chesscog-report/raw/master/report.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/blog/2021/chesscog" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/georg-wolflein/chesscog" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Identifying the configuration of chess pieces from an image of a chessboard is a problem in computer vision that has not yet been solved accurately. However, it is important for helping amateur chess players improve their games by facilitating automatic computer analysis without the overhead of manually entering the pieces. Current approaches are limited by the lack of large datasets and are not designed to adapt to unseen chess sets. This project puts forth a new dataset synthesised from a 3D model that is two orders of magnitude larger than existing ones. Trained on this dataset, a novel end-to-end chess recognition system is presented that combines traditional computer vision techniques with deep learning. It localises the chessboard using a RANSAC-based algorithm that computes a projective transformation of the board onto a regular grid. Using two convolutional neural networks, it then predicts an occupancy mask for the squares in the warped image and finally classifies the pieces. The described system achieves an error rate of 0.23% per square on the test set, 28 times better than the current state of the art. Further, a one-shot transfer learning approach is developed that is able to adapt the inference system to a previously unseen chess set using just two photos of the starting position, obtaining a per-square accuracy of 99.83% on images of that new chess set. Inference takes less than half a second on a GPU and about two seconds on a CPU. The feasibility of the system is demonstrated in an interactive web app available at https://www.chesscog.com.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://github.com/georg-wolflein/neural-surfing" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/neural-surfing.png"> <abbr class="badge">BSc</abbr> </a> </div> <div id="wolflein2020bsc" class="col-sm-7"> <div class="title"> Freeing Neural Training Through Surfing </div> <div class="author"> <em>Georg WÃ¶lflein</em> (supervised by <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/michael-kenneth-weir(609b0bc6-bbe9-4f05-bd90-92772b19acf9).html" target="_blank" rel="noopener noreferrer">Michael Weir</a>) </div> <div class="periodical"> <em>University of St Andrews</em>, 2020. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/georg-wolflein/neural-surfing/raw/master/report/report.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/neural-surfing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Gradient methods based on backpropagation are widely used in training multilayer feedforward neural networks. However, such algorithms often converge to suboptimal weight configurations known as local minima. This report presents a novel minimal example of the local minimum problem with only three training samples and demonstrates its suitability for investigating and resolving said problem by analysing its mathematical properties and conditions leading to the failure of conventional training regimes. A different perspective for training neural networks is introduced that concerns itself with neural spaces and is applied to study the local minimum example.This gives rise to the concept of setting intermediate subgoals during training which is demonstrated to be a viable and effective means of overcoming the local minimum problem. The versatility of subgoal-based approaches is highlighted by showing their potential for training more generally. An example of a subgoal-based training regime using sampling and an adaptive clothoid for establishing a goal-connecting path is suggested as a proof of concept for further research. In addition, this project includes the design and implementation of a software framework for monitoring the performance of different neural training algorithms on a given problem simultaneously and in real time. This framework can be used to reproduce the findings of how classical algorithms fail to find the global minimum in the aforementioned example.</p> </div> </div> </div></li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> Â© Copyright 2024 Georg WÃ¶lflein. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>