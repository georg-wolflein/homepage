<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Georg WÃ¶lflein</title> <meta name="author" content="Georg WÃ¶lflein"/> <meta name="description" content="Georg WÃ¶lflein's homepage. "/> <meta name="keywords" content="deep-learning, ai, ml, artificial-intelligence, machine-learning, digital-pathology"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://georg.woelflein.eu/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Georg_Wolflein_CV.pdf" target="_blank"> cv </a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Georg</span> WÃ¶lflein </h1> <p class="desc">PhD student in computer vision / deep learning</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Iâ€™m Georg, a PhD student in the <a href="https://cvrg.cs.st-andrews.ac.uk" target="_blank" rel="noopener noreferrer">Computer Vision Research Group</a> at the <a href="https://www.st-andrews.ac.uk" target="_blank" rel="noopener noreferrer">University of St Andrews</a>, advised by <a href="http://oa7.host.cs.st-andrews.ac.uk" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a>. Prior to this, I studied an integrated masterâ€™s (MSci) in Computer Science at the same institution. My research lies at the intersection of computer vision, deep learning, and computational pathology. In particular, I am interested in weakly supervised biomarker prediction in histopathology images, as well as self-supervised learning and vision-language models.</p> <p>Feel free to reach out via email!</p> <div class="social-links"> <a href="mailto:%67%65%6F%72%67@%77%6F%65%6C%66%6C%65%69%6E.%64%65"><i class="fas fa-envelope fa-lg"></i> email</a> <a href="https://scholar.google.com/citations?user=XTpEX9oAAAAJ" target="_blank" title="Google Scholar" rel="noopener noreferrer"><i class="ai ai-google-scholar ai-lg"></i> scholar</a> <a href="https://github.com/georg-wolflein" target="_blank" title="GitHub" rel="noopener noreferrer"><i class="fab fa-github fa-lg"></i> github</a> <a href="https://www.linkedin.com/in/georg-wolflein" target="_blank" title="LinkedIn" rel="noopener noreferrer"><i class="fab fa-linkedin fa-lg"></i> linkedin</a> </div> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">May 1, 2024</th> <td> I will be doing a research internship at Google London this summer. </td> </tr> <tr> <th scope="row">Feb 5, 2024</th> <td> I gave a <a href="https://warwick.ac.uk/fac/cross_fac/tia/seminars/seminars-23-24/#georg_wolflein" target="_blank" rel="noopener noreferrer">talk</a> at the Warwick Tissue Image Analytics (TIA) Centre about self-supervised feature extractors for pathology slide classification. </td> </tr> <tr> <th scope="row">Sep 10, 2022</th> <td> One paper was accepted at <a href="https://wacv2023.thecvf.com" target="_blank" rel="noopener noreferrer">WACV 2023</a>. </td> </tr> <tr> <th scope="row">May 26, 2021</th> <td> My first paper, <em>Determining Chess Game State from an Image</em>, was accepted at the <a href="https://www.mdpi.com/2313-433X/7/6/94" target="_blank" rel="noopener noreferrer">Journal of Imaging</a>! </td> </tr> <tr> <th scope="row">Jul 1, 2020</th> <td> Iâ€™m honoured to receive the <a href="https://info.cs.st-andrews.ac.uk/student-handbook/academic/prizes.html" target="_blank" rel="noopener noreferrer">Adobe Prize</a>. </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <p> Please refer to the <a href="/publications/">publications tab</a> or <a href="https://scholar.google.com/citations?user=XTpEX9oAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a> for a complete list. </p> <ol class="bibliography"> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://arxiv.org/abs/2311.11772v4" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/histaug.png"> <abbr class="badge">ECCV</abbr> </a> </div> <div id="wolflein2023good" class="col-sm-7"> <div class="title"> A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://www.klinikum.uni-heidelberg.de/personen/dyke-ferber-9789" target="_blank" rel="noopener noreferrer">Dyke Ferber</a>, <a href="https://scholar.google.com/citations?user=GnhtOGMAAAAJ" target="_blank" rel="noopener noreferrer">Asier Rabasco Meneghetti</a>, <a href="https://scholar.google.com/citations?user=tE9cPywAAAAJ" target="_blank" rel="noopener noreferrer">Omar S. M. El Nahhas</a>, <a href="https://www.lfb.rwth-aachen.de/en/institute/team/truhn/" target="_blank" rel="noopener noreferrer">Daniel Truhn</a>, <a href="https://scholar.google.com/citations?user=I_G3fPgAAAAJ" target="_blank" rel="noopener noreferrer">Zunamys I. Carrero</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J. Harrison</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a>, and <a href="https://kather.ai" target="_blank" rel="noopener noreferrer">Jakob Nikolas Kather</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em>, 2024. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.11772" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2311.11772v4.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/good-features" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.youtube.com/watch?v=Tst4XtaT9RE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://georg.woelflein.eu/good-features" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Stain normalisation is thought to be a crucial preprocessing step in computational pathology pipelines. We question this belief in the context of weakly supervised whole slide image classification, motivated by the emergence of powerful feature extractors trained using self-supervised learning on diverse pathology datasets. To this end, we performed the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 8,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Notably, we find that omitting stain normalisation and image augmentations does not compromise downstream slide-level classification performance, while incurring substantial savings in memory and compute. Using a new evaluation metric that facilitates relative downstream performance comparison, we identify the best publicly available extractors, and show that their latent spaces are remarkably robust to variations in stain and augmentations like rotation. Contrary to previous patch-level benchmarking studies, our approach emphasises clinical relevance by focusing on slide-level biomarker prediction tasks in a weakly supervised setting with external validation cohorts. Our findings stand to streamline digital pathology workflows by minimising preprocessing needs and informing the selection of feature extractors.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview "> <a href="https://arxiv.org/abs/2305.10552" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/das-mil.png"> </a> </div> <div id="wolflein2023deep" class="col-sm-7"> <div class="title"> Deep Multiple Instance Learning with Distance-Aware Self-Attention </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://caraml-group.github.io/author/lucie-charlotte-magister/" target="_blank" rel="noopener noreferrer">Lucie Charlotte Magister</a>, <a href="https://www.cl.cam.ac.uk/~pl219/" target="_blank" rel="noopener noreferrer">Pietro LiÃ²</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J Harrison</a>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> 2023. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.10552" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2305.10552.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/das-mil" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Traditional supervised learning tasks require a label for every instance in the training set, but in many real-world applications, labels are only available for collections (bags) of instances. This problem setting, known as multiple instance learning (MIL), is particularly relevant in the medical domain, where high-resolution images are split into smaller patches, but labels apply to the image as a whole. Recent MIL models are able to capture correspondences between patches by employing self-attention, allowing them to weigh each patch differently based on all other patches in the bag. However, these approaches still do not consider the relative spatial relationships between patches within the larger image, which is especially important in computational pathology. To this end, we introduce a novel MIL model with distance-aware self-attention (DAS-MIL), which explicitly takes into account relative spatial information when modelling the interactions between patches. Unlike existing relative position representations for self-attention which are discrete, our approach introduces continuous distance-dependent terms into the computation of the attention weights, and is the first to apply relative position representations in the context of MIL. We evaluate our model on a custom MNIST-based MIL dataset that requires the consideration of relative spatial information, as well as on CAMELYON16, a publicly available cancer metastasis detection dataset, where we achieve a test AUROC score of 0.91. On both datasets, our model outperforms existing MIL approaches that employ absolute positional encodings, as well as existing relative position representation schemes applied to MIL. Our code is available at https://anonymous.4open.science/r/das-mil.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://openaccess.thecvf.com/content/WACV2023/html/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.html" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hoechstgan.png"> <abbr class="badge">WACV</abbr> </a> </div> <div id="wolflein2023hoechstgan" class="col-sm-7"> <div class="title"> HoechstGAN: Virtual Lymphocyte Staining Using Generative Adversarial Networks </div> <div class="author"> <em>Georg WÃ¶lflein</em>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/in-hwa-um(0ac978a2-6ef8-4397-bc36-f920a77696a3).html" target="_blank" rel="noopener noreferrer">In Hwa Um</a>, <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/david-james-harrison(6bb6c114-15d1-4b0d-9091-8ce3ce9c2c7d).html" target="_blank" rel="noopener noreferrer">David J Harrison</a>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2023. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.06909" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/georg-wolflein/hoechstgan" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/hoechstgan_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a> <a href="https://www.youtube.com/watch?v=XrhwqRNjtv4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://georg.woelflein.eu/hoechstgan" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>The presence and density of specific types of immune cells are important to understand a patientâ€™s immune response to cancer. However, immunofluorescence staining required to identify T cell subtypes is expensive, timeconsuming, and rarely performed in clinical settings. We present a framework to virtually stain Hoechst images (which are cheap and widespread) with both CD3 and CD8 to identify T cell subtypes in clear cell renal cell carcinoma using generative adversarial networks. Our proposed method jointly learns both staining tasks, incentivising the network to incorporate mutually beneficial information from each task. We devise a novel metric to quantify the virtual staining quality, and use it to evaluate our method.</p> </div> </div> </div></li> <li><div class="row"> <div class="col-sm-3 preview abbr"> <a href="https://www.mdpi.com/2313-433X/7/6/94" target="_blank" rel="noopener noreferrer"> <img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/chesscog.png"> <abbr class="badge">J. Imaging</abbr> </a> </div> <div id="wolflein2021determining" class="col-sm-7"> <div class="title"> Determining Chess Game State from an Image </div> <div class="author"> <em>Georg WÃ¶lflein</em>, and <a href="https://risweb.st-andrews.ac.uk/portal/en/persons/oggie-arandelovic(fdd98ab1-564a-42a3-bf0c-fab7afbbd63c).html" target="_blank" rel="noopener noreferrer">Ognjen ArandjeloviÄ‡</a> </div> <div class="periodical"> <em>Journal of Imaging</em>, vol. 7, no. 6, 2021. </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2104.14963" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://www.mdpi.com/2313-433X/7/6/94/pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/blog/2021/chesscog" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/georg-wolflein/chesscog" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Identifying the configuration of chess pieces from an image of a chessboard is a problem in computer vision that has not yet been solved accurately. However, it is important for helping amateur chess players improve their games by facilitating automatic computer analysis without the overhead of manually entering the pieces. Current approaches are limited by the lack of large datasets and are not designed to adapt to unseen chess sets. This paper puts forth a new dataset synthesised from a 3D model that is an order of magnitude larger than existing ones. Trained on this dataset, a novel end-to-end chess recognition system is presented that combines traditional computer vision techniques with deep learning. It localises the chessboard using a RANSAC-based algorithm that computes a projective transformation of the board onto a regular grid. Using two convolutional neural networks, it then predicts an occupancy mask for the squares in the warped image and finally classifies the pieces. The described system achieves an error rate of 0.23% per square on the test set, 28 times better than the current state of the art. Further, a few-shot transfer learning approach is developed that is able to adapt the inference system to a previously unseen chess set using just two photos of the starting position, obtaining a per-square accuracy of 99.83% on images of that new chess set. The code, dataset, and trained models are made available online.</p> </div> </div> </div></li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%67%65%6F%72%67@%77%6F%65%6C%66%6C%65%69%6E.%64%65" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=XTpEX9oAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/georg-wolflein" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/georg-wolflein" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> Â© Copyright 2024 Georg WÃ¶lflein. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>